# ADAPTA: Advanced Data Analysis & Prediction for Health Assessment

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![GitHub stars](https://img.shields.io/github/stars/loxleyftsck/brfss-heart-disease-ml?style=social)](https://github.com/loxleyftsck/brfss-heart-disease-ml)

> ğŸ¥ **Machine Learning for Heart Disease Risk Prediction using CDC BRFSS 2020 Data**

## ğŸ“‹ Project Overview

**ADAPTA** is an academic data mining project focused on **predicting heart disease risk** using machine learning techniques applied to the **BRFSS 2020** (Behavioral Risk Factor Surveillance System) dataset from the CDC.

### Authors
- **Herald Michain Samuel Theo** (NIM: 215314017)
- **Fera Cisca Wanda Hamid** (NIM: 225314142)

### Institution
Data Mining Course - Advanced Machine Learning Applications

---

## ğŸ¯ Project Objectives

1. **Develop predictive models** for heart disease risk assessment
2. **Handle class imbalance** in medical datasets effectively
3. **Compare multiple ML algorithms** (Random Forest vs Logistic Regression)
4. **Optimize model performance** through hyperparameter tuning
5. **Provide interpretable results** for clinical decision support

---

## ğŸ“Š Dataset Description

### Source
- **Name**: Behavioral Risk Factor Surveillance System (BRFSS) 2020
- **Provider**: Centers for Disease Control and Prevention (CDC), USA
- **Size**: ~315 MB (331,045,949 bytes)
- **Records**: 400,000+ individuals
- **Features**: 279 variables (demographics, health status, lifestyle)

### Target Variable
- **`_MICHD`**: Myocardial Infarction or Coronary Heart Disease
  - `1`: Has history of heart disease
  - `0`: No history of heart disease

### Selected Features
| Feature | Description | Type |
|---------|-------------|------|
| `_SEX` | Gender | Categorical |
| `_AGEG5YR` | Age group (5-year intervals) | Ordinal |
| `_RFSMOK3` | Smoking status | Binary |
| `_RFBMI5` | BMI category | Categorical |
| `_TOTINDA` | Physical activity | Binary |

### Data Characteristics
- **Class Distribution**:
  - Negative (No disease): 260,246 (91%)
  - Positive (Has disease): 25,700 (9%)
- **Challenge**: Highly imbalanced dataset

---

## ğŸ”¬ Methodology

### 1. Data Preprocessing
```
Raw Data â†’ Cleaning â†’ Feature Selection â†’ Normalization â†’ Train/Test Split
```

**Steps**:
- Remove missing values (codes 7, 9 = Don't know/Refused)
- Select relevant health risk factors
- Apply StandardScaler for normalization
- Stratified split (70% train, 30% test)

### 2. Model Development

#### Models Evaluated
1. **Random Forest Classifier**
   - Ensemble method using decision trees
   - Handles non-linear relationships
   - Provides feature importance

2. **Logistic Regression**
   - Linear probabilistic model
   - Fast training and inference
   - Interpretable coefficients

#### Class Imbalance Handling
- Strategy: `class_weight='balanced'`
- Effect: Penalizes misclassification of minority class
- Alternative considered: SMOTE (not used due to risk of overfitting)

### 3. Hyperparameter Optimization

**Random Forest**:
```python
{
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 10],
    'class_weight': ['balanced']
}
```

**Logistic Regression**:
```python
{
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['lbfgs']
}
```

**Optimization Technique**: GridSearchCV with 5-fold cross-validation

---

## ğŸ“ˆ Results & Performance

### ğŸ¯ Model Performance Comparison

#### Overall Metrics (Test Set: 85,784 samples)

| Model Configuration | Precision | Recall | F1-Score | Accuracy | ROC-AUC |
|:-------------------|----------:|-------:|---------:|---------:|--------:|
| **Random Forest** (Baseline) | 0.18 | **0.81** | 0.30 | 0.66 | 0.74 |
| **Random Forest** (Tuned) | 0.18 | **0.81** | 0.30 | 0.66 | 0.74 |
| **Logistic Regression** (Baseline) | 0.18 | **0.80** | 0.30 | 0.66 | 0.73 |
| **Logistic Regression** (Tuned) | 0.18 | **0.80** | 0.30 | 0.66 | 0.73 |

> ğŸ“Š **Key Observation**: Hyperparameter tuning showed **no performance improvement**, indicating:
> - Default parameters were already optimal
> - Current feature set limits model ceiling
> - Linear patterns dominate (RF â‰ˆ LR performance)

---

### ğŸ“Š Detailed Performance Breakdown

#### Random Forest vs Logistic Regression

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PERFORMANCE COMPARISON                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Metric         â”‚  Random Forest  â”‚  Logistic Regression    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  Precision      â”‚      18%        â”‚        18%              â”‚
â”‚  Recall         â”‚      81% â­     â”‚        80% â­           â”‚
â”‚  F1-Score       â”‚      30%        â”‚        30%              â”‚
â”‚  Accuracy       â”‚      66%        â”‚        66%              â”‚
â”‚  Training Time  â”‚     ~4s         â”‚        ~0.5s            â”‚
â”‚  Interpretabilityâ”‚     Medium     â”‚        High âœ“           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend: â­ = Strong Performance  â”‚  âœ“ = Better Choice
```

**Winner**: **Logistic Regression** âœ…
- Same accuracy as Random Forest
- **8x faster** training time
- **More interpretable** coefficients
- Simpler model = easier deployment

---

### ğŸ” Confusion Matrix Analysis

#### Predictions on 85,784 Test Samples

**Random Forest**:
```
                  Predicted
                  Neg    Pos
Actual  Neg    50,000  28,074  â† 36% False Positive Rate
        Pos     1,469   6,241  â† 19% False Negative Rate
                          â†‘
                     81% Recall (Good!)
```

**Key Insights**:
- âœ… Catches **6,241 out of 7,710** heart disease cases (81% recall)
- âš ï¸ **28,074 false alarms** (low precision acceptable for screening)
- **Critical**: Only **1,469 missed diagnoses** (19% false negative)

---

### ğŸ† Best Model Configuration

**Recommended**: **Logistic Regression (Balanced)**

**Hyperparameters**:
```python
{
    "C": 0.01,              # Regularization strength
    "penalty": "l2",        # L2 regularization
    "solver": "lbfgs",      # Optimizer
    "class_weight": "balanced",  # Handle imbalance
    "random_state": 42      # Reproducibility
}
```

**Why This Model?**:
1. âš¡ **Fast**: 8x faster than Random Forest
2. ğŸ“Š **Interpretable**: Clear feature coefficients
3. ğŸ¯ **Effective**: 80% recall for disease detection
4. ğŸš€ **Simple**: Easy to deploy and maintain

---

### ğŸ’¡ Key Findings

#### âœ… Strengths
- **High Recall (80-81%)**: Detects 4 out of 5 heart disease cases
- **Consistent Performance**: Both models agree on predictions
- **Production Ready**: Fast inference time (< 1ms per prediction)

#### âš ï¸ Trade-offs
- **Low Precision (18%)**: 82% of positive predictions are false alarms
- **Acceptable for Screening**: Better to over-test than miss diagnoses
- **Requires Follow-up**: Positive results need clinical confirmation

#### ğŸ” Insights
- **Linear Patterns Dominate**: LR matches RF performance
- **Feature Engineering Needed**: Current 5 features limit ceiling
- **Class Imbalance Handled**: Balanced weights prevent majority-class bias

---

## ğŸš€ Getting Started

### Prerequisites
```bash
Python 3.8+
pip or conda
```

### Installation

1. **Clone the repository**:
```bash
git clone https://github.com/loxleyftsck/brfss-heart-disease-ml.git
cd brfss-heart-disease-ml
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Download dataset**:
```bash
# Place brfss2020.csv in data/raw/
# Or download from CDC: https://www.cdc.gov/brfss/
```

### Quick Start

#### Run Full Pipeline:
```bash
python src/main.py
```

#### Or Step-by-Step in Notebooks:
```bash
jupyter notebook
# Open notebooks/ and run sequentially:
# 01_exploration.ipynb â†’ 02_cleaning.ipynb â†’ ... â†’ 05_evaluation.ipynb
```

---

## ğŸ“ Project Structure

```
adapta-datamining/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”œâ”€â”€ LICENSE                      # MIT License
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Original dataset (do not modify)
â”‚   â”‚   â””â”€â”€ brfss2020.csv
â”‚   â”œâ”€â”€ processed/               # Cleaned data (generated by code)
â”‚   â”‚   â””â”€â”€ train_test_split.pkl
â”‚   â””â”€â”€ external/                # External datasets or references
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploration.ipynb     # EDA and data understanding
â”‚   â”œâ”€â”€ 02_cleaning.ipynb        # Data cleaning pipeline
â”‚   â”œâ”€â”€ 03_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 04_modeling.ipynb        # Model training
â”‚   â””â”€â”€ 05_evaluation.ipynb      # Results and metrics
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py           # Load raw data
â”‚   â”œâ”€â”€ preprocessing.py         # Cleaning and transformation
â”‚   â”œâ”€â”€ features.py              # Feature engineering
â”‚   â”œâ”€â”€ models.py                # Model definitions
â”‚   â”œâ”€â”€ evaluation.py            # Metrics and evaluation
â”‚   â”œâ”€â”€ utils.py                 # Helper functions
â”‚   â””â”€â”€ main.py                  # End-to-end pipeline
â”‚
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ results.csv              # Experiment tracking
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figures/                 # Plots and visualizations
â”‚   â”‚   â”œâ”€â”€ confusion_matrix_rf.png
â”‚   â”‚   â”œâ”€â”€ confusion_matrix_lr.png
â”‚   â”‚   â””â”€â”€ performance_comparison.png
â”‚   â””â”€â”€ final_report.pdf         # Academic report
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ methodology.md           # Detailed methodology
    â””â”€â”€ data_dictionary.md       # Feature descriptions
```

---

## ğŸ”§ Technologies Used

- **Python 3.9**
- **pandas**: Data manipulation
- **NumPy**: Numerical computing
- **scikit-learn**: Machine learning
- **matplotlib/seaborn**: Visualization
- **Jupyter**: Interactive notebooks

---

## ğŸ“ Reproducibility

All random processes are seeded for reproducibility:
```python
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
```

---

## ğŸ“š References

1. CDC. (2020). *Behavioral Risk Factor Surveillance System*. https://www.cdc.gov/brfss/
2. Chawla, N. V., et al. (2002). "SMOTE: Synthetic Minority Over-sampling Technique"
3. Breiman, L. (2001). "Random Forests". *Machine Learning*, 45(1), 5-32.

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Centers for Disease Control and Prevention (CDC) for providing the BRFSS dataset
- Course instructors for guidance and support
- Open-source community for Python libraries

---

**Last Updated**: January 2026
